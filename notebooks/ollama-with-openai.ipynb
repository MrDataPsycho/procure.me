{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert ollama-with-openai.ipynb --to python --output ../src/smart_procurement/models/chat_interface.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"gemma3:4b\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant designed to match user queries with relevant commodity codes with descriptions as context used in procurement purchase orders.\n",
    "You will go through the description provided for each commodity code and try to match the user's query with atleast top 3 relevant commodity codes.\n",
    "You must always respond with a valid JSON array of objects where each object contains:\n",
    "1. \"code\": The ID of the commodity code (as a string).\n",
    "2. \"confidence\": A number from 0 to 100 indicating your confidence that the commodity code matches the user's query.\n",
    "3. code and confidence both should be integers.\n",
    "4. You should go through all the descriptions and then decide which commodity code matches the user's query.\n",
    "\n",
    "The JSON format should look like this wrapped with markdown code block:\n",
    "```json\n",
    "[\n",
    "    {\"code\": 10001, \"confidence\": 80},\n",
    "    {\"code\": 10002, \"confidence\": 60},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "Only include the commodity codes that have a relevance or confidence above 30. \n",
    "If no commodity code matches the query, return an empty array.\n",
    "\n",
    "Always ensure the JSON response is valid.\n",
    "After you prepared the json you must validate it that is a valid json and return it to user.\n",
    "Provide me only the JSON and nothing else. Do not add any extra text in the beginning or end of the JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"\n",
    "1. 10001 - Computer, Hardware\n",
    "2. 10002 - Software for Computer\n",
    "3. 10003 - Office Furniture\n",
    "4. 10004 - Computer Accessories\n",
    "5. 10005 - Office Supplies\n",
    "\"\"\"\n",
    "\n",
    "QUERY = \"I want to buy a keyboard and mouse.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = f\"\"\"Query:\\n{QUERY} \\n\\n Context:\\n{CONTEXT}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", \n",
    "        content=SYSTEM_PROMPT,\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=USER_PROMPT),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n    {\"code\": 10004, \"confidence\": 95},\\n    {\"code\": 10001, \"confidence\": 60},\\n    {\"code\": 10005, \"confidence\": 30}\\n]\\n```'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_markdown(markdown_content):\n",
    "    # Regular expression to find JSON content between markdown code block\n",
    "    json_pattern = r\"```json\\n(.*?)\\n```\"\n",
    "    match = re.search(json_pattern, markdown_content, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_content = match.group(1)\n",
    "        try:\n",
    "            # Parse and return the JSON object\n",
    "            return json.loads(json_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 10004, 'confidence': 95},\n",
       " {'code': 10001, 'confidence': 60},\n",
       " {'code': 10005, 'confidence': 30}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_from_markdown(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommodityCodeResult(BaseModel):\n",
    "    code: int\n",
    "    confidence: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommodityCodeResultList(BaseModel):\n",
    "    results: list[CommodityCodeResult]\n",
    "\n",
    "    def get_all_codes(self) -> list[int]:\n",
    "        if self.results:\n",
    "            return [int(item.code) for item in self.results]\n",
    "        return []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_model_response_str(cls, response_str: str) -> Self:\n",
    "        response_dict = extract_json_from_markdown(response_str)\n",
    "        if response_dict:\n",
    "            return cls(results=[CommodityCodeResult(**item) for item in response_dict])\n",
    "        return cls(results=[])\n",
    "    \n",
    "    def get_key_value_pairs(self) -> dict[int, int]:\n",
    "        return {item.code: item.confidence for item in self.results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodit_codes = CommodityCodeResultList.from_model_response_str(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10004, 10005]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodit_codes.get_all_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class ModelSelection(StrEnum):\n",
    "    \"\"\"Enum for selecting the available LLM models.\"\"\"\n",
    "    GEMMA3 = \"gemma3:4b\"\n",
    "    DEEPSEEKR1 = \"deepseek-r1:7b\"\n",
    "    NOMIC = \"nomic-embed-text:v1.5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"\n",
    "PharmaCorp, a global pharmaceutical giant, has finalized a landmark Master Level Agreement \n",
    "with ProcureMe, a leading provider of intelligent procurement solutions leveraging SAP Ariba \n",
    "Contracts. The agreement marks a significant step in PharmaCorp’s digital transformation \n",
    "strategy, aimed at streamlining its sourcing processes and accelerating innovation. Under the \n",
    "terms of the agreement, PharmaCorp will be outsourcing the development and maintenance of its \n",
    "Gen AI-powered procurement tools to a dedicated team of 3 Gen AI developers staffed by \n",
    "ProcureMe. This partnership represents a commitment to embracing cutting-edge technology and \n",
    "optimizing operational efficiency.\n",
    "\n",
    "The core of the agreement revolves around ProcureMe's expertise in utilizing SAP Ariba \n",
    "Contracts, an established platform adopted by numerous Fortune 500 companies. The Gen AI \n",
    "developers, working alongside PharmaCorp’s procurement team, will focus on customizing the \n",
    "Ariba Contracts platform with Gen AI capabilities. This includes developing automated contract \n",
    "review tools, intelligent spend analysis, and proactive risk mitigation strategies. The \n",
    "initial phase of the project will concentrate on optimizing PharmaCorp’s supplier onboarding \n",
    "process, significantly reducing administrative overhead.\n",
    "\n",
    "ProcureMe’s selection was driven by the company’s deep understanding of large enterprise \n",
    "procurement workflows and its proven track record of delivering impactful Gen AI solutions. \n",
    "The partnership integrates seamlessly with PharmaCorp’s existing IT infrastructure and \n",
    "utilizes a secure, cloud-based environment, ensuring data privacy and compliance with industry \n",
    "regulations. Furthermore, the contract includes robust Service Level Agreements (SLAs) \n",
    "guaranteeing performance, uptime, and ongoing support.\n",
    "\n",
    "Both companies view this collaboration as a catalyst for future innovation.  PharmaCorp \n",
    "anticipates that the Gen AI-powered solutions will not only reduce operational costs but also \n",
    "empower its procurement team to make more data-driven decisions.  ProcureMe, in turn, gains \n",
    "valuable exposure to a complex, regulated industry and an opportunity to refine its Gen AI \n",
    "offerings for large-scale enterprise deployments. The long-term goal is to foster a strategic \n",
    "relationship built on continuous improvement and collaborative innovation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"What is the master level agreement between PharmaCorp and ProcureMe?\",\n",
    "    \"What are the key features of the Gen AI-powered procurement tools?\",\n",
    "    \"How does PharmaCorp optimize its supplier onboarding process?\",\n",
    "    \"What are the SLAs included in the contract?\",\n",
    "    \"What are the long-term goals of the partnership?\",\n",
    "    \"What abc company is buying from DEF company?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_AUGMENTED_GENERATION = \"\"\"\n",
    "You are a helpful and informative assistant powered by a Retrieval-Augmented Generation (RAG) \n",
    "system. Your primary goal is to answer user questions accurately and concisely, leveraging a \n",
    "provided context document. You will output your responses in JSON format.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Receive Context:** You will be provided with a document (referred to as \n",
    "\"[CONTEXT_DOCUMENT_NAME]\") containing relevant information. *Carefully read and understand \n",
    "this document.*\n",
    "\n",
    "2.  **Receive Query:** You will also receive a user's question (referred to as \n",
    "\"[USER_QUERY]\").\n",
    "\n",
    "3.  **Retrieve Relevant Information:** Based on the [USER_QUERY], identify the most relevant \n",
    "segments of the [CONTEXT_DOCUMENT_NAME]. Focus on extracting direct answers and supporting \n",
    "details.\n",
    "\n",
    "4.  **Generate Answer:** Synthesize the retrieved information into a clear, concise, and \n",
    "accurate answer to the [USER_QUERY].\n",
    "\n",
    "5.  **Output JSON Response:** Format your response as a JSON object with the following \n",
    "structure:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "      \"response\": \"[Generated Answer with Citation Numbering]\",\n",
    "      \"sources\": [\n",
    "        {\n",
    "          \"document_title\": \"[CONTEXT_DOCUMENT_NAME]\",\n",
    "          \"section_text\": \"[Relevant Section/Paragraph Text]\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    *   `response`:  The generated answer to the user's question, including the citation \n",
    "number. Example: “The agreement assigns 3 Gen AI developers. [1]”\n",
    "    *   `sources`: An array containing details about the source of the information. Each \n",
    "object in the array should include:\n",
    "        *   `document_title`: The title of the document where the answer was found.\n",
    "        *   `section_text`:  The exact text from the document that supports the answer.\n",
    "        *   `success`: true/false based on either answered the question or not\n",
    "\n",
    "6.  **Maintain Professional Tone:** Respond in a professional and informative tone.\n",
    "\n",
    "7. **If the answer cannot be found within the context document, respond with:** “I’m sorry, I \n",
    "cannot answer that question based on the provided context document.” \n",
    "\n",
    "8. ** the answer cannot be found within the context:\n",
    "   - `sources` should be empty list: Example: sources: []\n",
    "   - success should be false Example: success: false\n",
    "   - No numbering will be added into the response\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Success Example:\n",
    "*   [CONTEXT_DOCUMENT_NAME]: PharmaCorp and ProcureMe Agreement\n",
    "*   [USER_QUERY]: How many Gen AI developers will ProcureMe be assigning to PharmaCorp?\n",
    "\n",
    "*   Response:\n",
    "    ```json\n",
    "    {\n",
    "      \"response\": \"The agreement assigns 3 Gen AI developers. [1]\",\n",
    "      \"success\": true,\n",
    "      \"sources\": [\n",
    "        {\n",
    "          \"document_title\": \"PharmaCorp and ProcureMe Agreement\",\n",
    "          \"section_text\": \"ProcureMe will be assigning 3 Gen AI developers to support \n",
    "PharmaCorp’s procurement needs.\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    ```\n",
    "Fail Example:\n",
    "*   [CONTEXT_DOCUMENT_NAME]: PharmaCorp and ProcureMe Agreement\n",
    "*   [USER_QUERY]: How many Gen AI developers will LargeCorp be assigning to XCorp?\n",
    "  ``json\n",
    "      {\n",
    "        \"response\": \"I’m sorry, I cannot answer that question based on the provided context document.\",\n",
    "        \"success\": false,\n",
    "        \"sources\": []\n",
    "      }\n",
    "  ```\n",
    "\n",
    "**Important:** Always adhere to the specified JSON format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the master level agreement between PharmaCorp and ProcureMe?',\n",
       " 'What are the key features of the Gen AI-powered procurement tools?',\n",
       " 'How does PharmaCorp optimize its supplier onboarding process?',\n",
       " 'What are the SLAs included in the contract?',\n",
       " 'What are the long-term goals of the partnership?',\n",
       " 'What abc company is buying from DEF company?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = f\"\"\"Query:\\n{QUESTIONS[5]} \\n\\n Context:\\n{CONTEXT}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", \n",
    "        content=SYSTEM_PROMPT_AUGMENTED_GENERATION,\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=USER_PROMPT),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"response\": \"The document does not specify which company ABC is buying from DEF company.\",\\n  \"success\": false,\\n  \"sources\": []\\n}\\n```'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short story in 3 lines.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Here is 1 line. Here is line 2. Here is line 3.\"},\n",
    "        {\"role\": \"user\", \"content\": \"The story you told me was is good enough? Yes or no?\"},\n",
    "    ]\n",
    "    response = ollama.chat(\n",
    "        messages=messages,\n",
    "        model=\"gemma3:4b\",\n",
    "        stream=True\n",
    "    )\n",
    "    # Process the response as needed\n",
    "    for chunk in response:\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end='', flush=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
