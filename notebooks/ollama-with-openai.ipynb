{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ollama-with-openai.ipynb to python\n",
      "[NbConvertApp] Writing 3944 bytes to ../src/smart_procurement/models/chat_interface.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert ollama-with-openai.ipynb --to python --output ../src/smart_procurement/models/chat_interface.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from typing import Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant designed to match user queries with relevant commodity codes with descriptions as context used in procurement purchase orders.\n",
    "You will go through the description provided for each commodity code and try to match the user's query with atleast top 3 relevant commodity codes.\n",
    "You must always respond with a valid JSON array of objects where each object contains:\n",
    "1. \"code\": The ID of the commodity code (as a string).\n",
    "2. \"confidence\": A number from 0 to 100 indicating your confidence that the commodity code matches the user's query.\n",
    "3. code and confidence both should be integers.\n",
    "4. You should go through all the descriptions and then decide which commodity code matches the user's query.\n",
    "\n",
    "The JSON format should look like this wrapped with markdown code block:\n",
    "```json\n",
    "[\n",
    "    {\"code\": 10001, \"confidence\": 80},\n",
    "    {\"code\": 10002, \"confidence\": 60},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "Only include the commodity codes that have a relevance or confidence above 30. \n",
    "If no commodity code matches the query, return an empty array.\n",
    "\n",
    "Always ensure the JSON response is valid.\n",
    "After you prepared the json you must validate it that is a valid json and return it to user.\n",
    "Provide me only the JSON and nothing else. Do not add any extra text in the beginning or end of the JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT = \"\"\"\n",
    "1. 10001 - Computer, Hardware\n",
    "2. 10002 - Software for Computer\n",
    "3. 10003 - Office Furniture\n",
    "4. 10004 - Computer Accessories\n",
    "5. 10005 - Office Supplies\n",
    "\"\"\"\n",
    "\n",
    "QUERY = \"I want to buy a keyboard and mouse.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = f\"\"\"Query:\\n{QUERY} \\n\\n Context:\\n{CONTEXT}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", \n",
    "        content=SYSTEM_PROMPT,\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=USER_PROMPT),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n    {\"code\": 10001, \"confidence\": 20},\\n    {\"code\": 10004, \"confidence\": 90},\\n    {\"code\": 10002, \"confidence\": 0}\\n]\\n```'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_markdown(markdown_content):\n",
    "    # Regular expression to find JSON content between markdown code block\n",
    "    json_pattern = r\"```json\\n(.*?)\\n```\"\n",
    "    match = re.search(json_pattern, markdown_content, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        json_content = match.group(1)\n",
    "        try:\n",
    "            # Parse and return the JSON object\n",
    "            return json.loads(json_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'code': 10001, 'confidence': 20},\n",
       " {'code': 10004, 'confidence': 90},\n",
       " {'code': 10002, 'confidence': 0}]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_from_markdown(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommodityCodeResult(BaseModel):\n",
    "    code: int\n",
    "    confidence: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommodityCodeResultList(BaseModel):\n",
    "    results: list[CommodityCodeResult]\n",
    "\n",
    "    def get_all_codes(self) -> list[int]:\n",
    "        if self.results:\n",
    "            return [int(item.code) for item in self.results]\n",
    "        return []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_model_response_str(cls, response_str: str) -> Self:\n",
    "        response_dict = extract_json_from_markdown(response_str)\n",
    "        if response_dict:\n",
    "            return cls(results=[CommodityCodeResult(**item) for item in response_dict])\n",
    "        return cls(results=[])\n",
    "    \n",
    "    def get_key_value_pairs(self) -> dict[int, int]:\n",
    "        return {item.code: item.confidence for item in self.results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "commodit_codes = CommodityCodeResultList.from_model_response_str(resp.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10004, 10005]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodit_codes.get_all_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phi3.5:latest'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class ModelSelection(StrEnum):\n",
    "    \"\"\"Enum for selecting the available LLM models.\"\"\"\n",
    "    PHI3_5 = \"phi3.5:latest\"\n",
    "    LLAMA3_1 = \"llama3.1:latest\"\n",
    "\n",
    "\n",
    "str(ModelSelection.PHI3_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-procurement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
