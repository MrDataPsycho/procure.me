{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "9f4f0630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from procureme.vectordb.lance_vectordb import LanceDBVectorStore\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from procureme.clients.ollama_embedder import OllamaEmbeddingClient\n",
    "import json\n",
    "from enum import StrEnum\n",
    "import logging\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d2bd5c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"AppAgent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "12387d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AiModels(StrEnum):\n",
    "    GEMMA3 = \"gemma3:4b\"\n",
    "    GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "    GPT4_1_NANO = \"gpt-4.1-nano\"\n",
    "    GPT4_1_MINI = \"gpt-4.1-mini-2025-04-14\"\n",
    "    NOMIC = \"nomic-embed-text:v1.5\"\n",
    "    LLAMA31 = \"llama3.1:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "92a42a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "2fbe66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e0894d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "5a3a0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"what do you know about Retrival Augmented Generation RAG? tell me in 1 sentence\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4960d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:59:26,957 - httpx - INFO - HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) combines the strengths of large language models with external knowledge retrieval systems to generate more accurate, contextually relevant, and informed responses.\n"
     ]
    }
   ],
   "source": [
    "response = ollama_client.chat.completions.create(\n",
    "  model=AiModels.GEMMA3,\n",
    "  messages=test_message\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "59fba0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:59:27,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Retrieval-Augmented Generation) is a natural language processing model that combines a retriever and a generator to improve text generation by first retrieving relevant information and then generating responses based on that information.\n"
     ]
    }
   ],
   "source": [
    "response = oai_client.chat.completions.create(\n",
    "  model=AiModels.GPT3,\n",
    "  messages=test_message\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a586faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store():\n",
    "    db_path = Path().absolute().parent.joinpath(\"vectordb\", \"contracts.db\")\n",
    "    vector_table = \"contracts_naive\"\n",
    "    emb_client = OllamaEmbeddingClient()\n",
    "    vector_store = LanceDBVectorStore(db_path=db_path, table_name=vector_table, embedding_client=emb_client)\n",
    "    logger.info(\"Vector DB Loaded successfully.\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "9ee34918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 23:59:27,783 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-01 23:59:27,787 - AppAgent - INFO - Vector DB Loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "VECTOR_STORE = get_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "7ac6dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 00:06:54,168 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chunk_id': 'CW0303.pdf-part - 4',\n",
       " 'doc_id': 'c95b4916-7170-422e-8371-1d7afa17fc25',\n",
       " 'file_name': 'CW0303.pdf',\n",
       " 'total_pages': 4,\n",
       " 'content': '................................................................. (Supplier legal entity name) that are in effect \\n(“Agreement”). \\nWith the signing of this document, we intend to amend the Agreement in order to fully \\nincorporate the above Environmental Sustainability Criteria. \\n................................................................. (Supplier legal entity name) at \\n........................................................................................... (Supplier address), acknowledges the \\nre-ceipt of the Plasma Corporation’ Environmental Sustainability Criteria and agrees to be bound \\nby them. \\nThis document shall be deemed part of the Agreement and any future reference to the Agreement \\nshall include the Environmental Sustainability Criteria. \\n \\n \\n[Signatures] \\n________________________ \\nJohn Doe \\nProcurement Manager \\nPLASMA Corporation \\n[Date] \\n________________________ \\nJane Smith \\nSales Manager \\nAlpha Suppliers Inc. \\n[Date] \\n',\n",
       " 'part': 'part - 4',\n",
       " '_distance': 316.0123291015625}"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?\"\n",
    "VECTOR_STORE.search(query=query, top_k=5)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "7047a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages, model=AiModels.GPT3, temperature=0, config={}):\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "        **config,\n",
    "    )\n",
    "    logger.info(f\"Chat endpoint called with Model {model}, Request: {messages[-1]}\")\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "aa30ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_choice(messages, model=AiModels.GPT4_1_MINI, temperature=0, tools=[], config={}):\n",
    "    response = oai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=messages,\n",
    "        tools=tools or None,\n",
    "        **config,\n",
    "    )\n",
    "    tools = response.choices[0].message.tool_calls\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f02bf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_update_prompt = \"\"\"\n",
    "    You are an expert at updating questions to make them ask for one thing only, more atomic, specific and easier to find the answer for.\n",
    "    You do this by filling in missing information in the question, with the extra information provided to you in previous answers. \n",
    "    \n",
    "    You respond with the updated question that has all information in it.\n",
    "    Only edit the question if needed. If the original question already is atomic, specific and easy to answer, you keep the original.\n",
    "    Do not ask for more information than the original question. Only rephrase the question to make it more complete.\n",
    "    \n",
    "    JSON template to use:\n",
    "    {\n",
    "        \"question\": \"question1\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "def query_update(input: str, answers: list[any]) -> str:\n",
    "    logger.info(\"=== Entering Query Update Node ===\")\n",
    "    logger.info(f\"Query Update endpoint called with Intermediate User and Assistant turns: {len(answers)}\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": query_update_prompt},\n",
    "        *answers,\n",
    "        {\"role\": \"user\", \"content\": f\"The user question to rewrite: '{input}'\"},\n",
    "    ]\n",
    "\n",
    "    config = {\"response_format\": {\"type\": \"json_object\"}}\n",
    "    output = chat(messages, model = AiModels.GPT4_1_MINI, config=config, )\n",
    "    try:\n",
    "        updated_question = json.loads(output)[\"question\"]\n",
    "        logger.info(f\"Updated Query: {updated_question}\")\n",
    "        return updated_question\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "a3fb5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_update(\"What do you know about Supplier: Alpha Suppliers Inc. and what they usually supply?\", answers=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53f6ee",
   "metadata": {},
   "source": [
    "# Tools and Their Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "d8d26ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_given_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"answer_given\",\n",
    "        \"description\": \"If the conversation already contains a complete answer to the question, \"\n",
    "        \"use this tool to extract it. Additionally, if the user engages in small talk, \"\n",
    "        \"use this tool to remind them that you can only answer questions about movies and their cast.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Respond directly with the answer\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"answer\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def answer_given(answer: str):\n",
    "    \"\"\"Extract the answer from a given text.\"\"\"\n",
    "    logger.info(\"Answer found in text: %s\", answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "551b0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver_tool_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"retriver_tool\",\n",
    "        \"description\": \"Query the vector database with a user question to pull the most relevant chunks. When other tools don't fit, fallback to use this one.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"question\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The user question or query to find the answer for\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"question\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def retriver_tool(question: str):\n",
    "    \"\"\"Query the database with a user question.\"\"\"\n",
    "    logger.info(\"=== Entering Retrival Tool ===\")\n",
    "    try:\n",
    "        vector_store = get_vector_store()\n",
    "        documents = vector_store.search(query=question, top_k=5)\n",
    "        context = \"\\n\\n\".join([doc[\"content\"] for doc in documents])\n",
    "        logger.info(\"Retrival Tool executed successfully.\")\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        return [f\"Could not query vector store, cause an error: {e}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "3480a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "    \"retriver_tool\": {\n",
    "        \"description\": retriver_tool_description,\n",
    "        \"function\": retriver_tool\n",
    "    },\n",
    "    \"answer_given\": {\n",
    "        \"description\": answer_given_description,\n",
    "        \"function\": answer_given\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf28c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_picker_prompt = \"\"\"\n",
    "    Your job is to chose the right tool needed to respond to the user question. \n",
    "    The available tools are provided to you in the prompt.\n",
    "    Make sure to pass the right and the complete arguments to the chosen tool.\n",
    "\"\"\"\n",
    "\n",
    "def handle_tool_calls(tools: dict[str, any], llm_tool_calls: list[dict[str, any]]):\n",
    "    logger.info(\"=== Selecting Tools ===\")\n",
    "    output = []\n",
    "    available_tools = [tool[\"description\"][\"function\"][\"name\"] for tool in tools.values()]\n",
    "    if llm_tool_calls:\n",
    "        tool_list = [tool_call.function.name for tool_call in llm_tool_calls]\n",
    "        logger.info(f\"Follwing tools are select for execution: {tool_list} from available tools: {available_tools}\")\n",
    "        for tool_call in llm_tool_calls:\n",
    "            function_to_call = tools[tool_call.function.name][\"function\"]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            res = function_to_call(**function_args)\n",
    "            output.append(res)\n",
    "    logger.info(f\"Tool execution finished!\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2a3068c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(question: str, tools: dict[str, any], answers: list[dict[str, str]]):\n",
    "    logger.info(\"=== Entering Tool Selector Router ===\")\n",
    "    llm_tool_calls = tool_choice(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": tool_picker_prompt,\n",
    "            },\n",
    "            *answers,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The user question to find a tool to answer: '{question}'\",\n",
    "            },\n",
    "        ],\n",
    "        model = AiModels.GPT4_1_MINI,\n",
    "        tools=[tool[\"description\"] for tool in tools.values()],\n",
    "    )\n",
    "    return handle_tool_calls(tools, llm_tool_calls)\n",
    "\n",
    "def handle_user_input(input: str, answers: Optional[list[dict[str, str]]] = None):  \n",
    "    answers = answers if answers else []\n",
    "    logger.info(f\"User input: {input}, with initial No. of User and Assistant turns: {len(answers)}\")\n",
    "    updated_question = query_update(input, answers)\n",
    "\n",
    "    response  = route_question(updated_question, tools, answers)\n",
    "    answers.append({\"role\": \"assistant\", \"content\": f\"For the question: '{updated_question}', we have the answer: '{json.dumps(response)}'\"})\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "5ada46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_critique_prompt = \"\"\"\n",
    "    You are an expert at identifying if questions has been fully answered or if there is an opportunity to enrich the answer.\n",
    "    The user will provide a question, and you will scan through the provided information to see if the question is answered.\n",
    "    If anything is missing from the answer, you will provide a set of new questions that can be asked to gather the missing information.\n",
    "    All new questions must be complete, atomic and specific.\n",
    "    However, if the provided information is enough to answer the original question, you will respond with an empty list.\n",
    "\n",
    "    JSON template to use for finding missing information:\n",
    "    {\n",
    "        \"questions\": [\"question1\", \"question2\"]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "def critique_answers(question: str, answers: list[dict[str, str]]) -> list[str]:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": answer_critique_prompt,\n",
    "        },\n",
    "        *answers,\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"The original user question to answer: {question}\",\n",
    "        },\n",
    "    ]\n",
    "    config = {\"response_format\": {\"type\": \"json_object\"}}\n",
    "    output = chat(messages, model=AiModels.GPT3, config=config)\n",
    "    logger.info(f\"Answer critique response: {output}\")\n",
    "    try:\n",
    "        return json.loads(output)[\"questions\"]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "23e19c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"\"\"\n",
    "    Your job is to help the user with their questions.\n",
    "    You will receive user questions and information needed to answer the questions\n",
    "    If the information is missing to answer part of or the whole question, you will say that the information \n",
    "    is missing. You will only use the information provided to you in the prompt to answer the questions.\n",
    "    You are not allowed to make anything up or use external information.\n",
    "\"\"\"\n",
    "\n",
    "def main(input: str):\n",
    "    answers = handle_user_input(input)\n",
    "    critique = critique_answers(input, answers)\n",
    "\n",
    "    if critique:\n",
    "        answers = handle_user_input(\" \".join(critique), answers)\n",
    "\n",
    "    llm_response = chat(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": main_prompt},\n",
    "            *answers,\n",
    "            {\"role\": \"user\", \"content\": f\"The user question to answer: {input}\"},\n",
    "        ],\n",
    "        model=AiModels.GPT4_1_MINI,\n",
    "    )\n",
    "\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "3d112cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = main(\"Who's the main actor in the movie Matrix and what other movies is that person in?\")\n",
    "# print(f\"Main response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "93eb3837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 00:00:22,511 - AppAgent - INFO - User input: What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?, with initial No. of User and Assistant turns: 0\n",
      "2025-06-02 00:00:22,513 - AppAgent - INFO - === Entering Query Update Node ===\n",
      "2025-06-02 00:00:22,514 - AppAgent - INFO - Query Update endpoint called with Intermediate User and Assistant turns: 0\n",
      "2025-06-02 00:00:23,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:23,672 - AppAgent - INFO - Chat endpoint called with Model gpt-4.1-mini-2025-04-14, Request: {'role': 'user', 'content': \"The user question to rewrite: 'What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?'\"}\n",
      "2025-06-02 00:00:23,674 - AppAgent - INFO - Updated Query: What products or services does Alpha Suppliers Inc. provide as a supplier?\n",
      "2025-06-02 00:00:23,675 - AppAgent - INFO - === Entering Tool Selector Router ===\n",
      "2025-06-02 00:00:24,529 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:24,531 - AppAgent - INFO - === Selecting Tools ===\n",
      "2025-06-02 00:00:24,532 - AppAgent - INFO - Follwing tools are select for execution: ['retriver_tool'] from available tools: ['retriver_tool', 'answer_given']\n",
      "2025-06-02 00:00:24,532 - AppAgent - INFO - === Entering Retrival Tool ===\n",
      "2025-06-02 00:00:24,638 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:24,641 - AppAgent - INFO - Vector DB Loaded successfully.\n",
      "2025-06-02 00:00:24,664 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:24,671 - AppAgent - INFO - Retrival Tool executed successfully.\n",
      "2025-06-02 00:00:24,673 - AppAgent - INFO - Tool executed successfully!\n",
      "2025-06-02 00:00:25,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:25,206 - AppAgent - INFO - Chat endpoint called with Model gpt-3.5-turbo-0125, Request: {'role': 'user', 'content': 'The original user question to answer: What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?'}\n",
      "2025-06-02 00:00:25,207 - AppAgent - INFO - Answer critique response: {\n",
      "    \"questions\": []\n",
      "}\n",
      "2025-06-02 00:00:26,534 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-02 00:00:26,537 - AppAgent - INFO - Chat endpoint called with Model gpt-4.1-mini-2025-04-14, Request: {'role': 'user', 'content': 'The user question to answer: What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main response: Alpha Suppliers Inc. is a supplier engaged under a Procurement Contract with XYZ Corporation. According to the contract details, Alpha Suppliers Inc. provides office stationery and supplies, including items such as pens, paper, folders, and other stationery items as specified in the attached Purchase Order. Additionally, they supply any additional stationery items requested by the Buyer during the contract period.\n"
     ]
    }
   ],
   "source": [
    "response = main(\"What do you know about Supplier: Alpha Suppliers Inc. and what it supplies?\")\n",
    "print(f\"Main response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d77098",
   "metadata": {},
   "source": [
    "# Pulling Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16575d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
